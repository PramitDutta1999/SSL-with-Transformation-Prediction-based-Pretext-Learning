{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxneI6uIojks"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/transformed_oct.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "7sSghDi0ozzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, BatchNormalization, ReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, Callback"
      ],
      "metadata": {
        "id": "2XNXZoN4o2KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/Downstream Task/train'\n",
        "val_dir = '/content/Downstream Task/validation'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create the train generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create the validation generator\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=62,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "AKRFSh7co9v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights=None, include_top=False, input_shape=(156, 156, 3))\n",
        "\n",
        "# Freeze layers\n",
        "for layer in base_model.layers[:50]:\n",
        "     layer.trainable = False\n",
        "\n",
        "# Set trainable\n",
        "for layer in base_model.layers[50:]:\n",
        "     layer.trainable = True\n",
        "\n",
        "# New layers\n",
        "\n",
        "#x = base_model.get_layer(\"conv3_block4_out\").output\n",
        "x = base_model.layers[-1].output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(4, activation='softmax')(x)  # 4 output classes\n",
        "\n",
        "# Define model\n",
        "downstream_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Load weights\n",
        "downstream_model.load_weights('/content/drive/MyDrive/weights/Full_Model/pretext_task_weights.weights.h5',skip_mismatch=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2,\n",
        "                              patience=2, min_lr=0.000001,verbose=1)\n",
        "\n",
        "# Compile the model for the downstream task\n",
        "downstream_model.compile(\n",
        "    optimizer= tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'auc']\n",
        ")"
      ],
      "metadata": {
        "id": "R9VsC0FVo-1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on labeled data\n",
        "history = downstream_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    verbose = 1,\n",
        "    callbacks=[reduce_lr],\n",
        ")"
      ],
      "metadata": {
        "id": "UNxCMp55phUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = '/content/drive/MyDrive/weights/Full_Model/downstream_task_weights.weights.h5'\n",
        "downstream_model.save_weights(weights_path)\n",
        "\n",
        "print(f\"Weights saved at: {weights_path}\")"
      ],
      "metadata": {
        "id": "ltdBGZGhrTIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terminate\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "QAyu1qmKrecI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}